{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\"\"\"\n",
    "This file is implementing the vanilla LSTM.\n",
    "The test set used are the 31 days of December of a single time-serie of the 261 time-series with measurements of the full year 2017.\n",
    "Missing days are estimated.\n",
    "Naive basemodels that are used are the mean forecast and the MAPE-minimization.\n",
    "Evaluation metrics used: MSE, RMSE, NRMSE, MAE\n",
    "####################################################\n",
    "\n",
    "calculate the MSE on the total test set and for each day of the week.\n",
    "Should normalize history and temperature with min max\n",
    "Rest: weekday, time, holiday use one hot encoder\n",
    "Stack different LSTM blocks --> hidden nodes serve as inputs for next LSTM's\n",
    "Use finally a fully connected layer to generate an output\n",
    "Use different learning rules: adam!!, stochastic gradient descent, Adagrad, Adadelta and RMSProp\n",
    "Adam has parameters: learning rate, momentum and decay\n",
    "play with mini batches\n",
    "\n",
    "Inputs:\n",
    "1. history ok\n",
    "2. temp ok\n",
    "3. day week ok\n",
    "4. time ok\n",
    "5. holiday\n",
    "6. (previous week future lag)\n",
    "7. previous week error history lag --> how good are you following previous week\n",
    "just the difference between the values\n",
    "8. previous weeks load at the same moment in time and the temperatures of these days\n",
    "\"\"\"\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nThis file is implementing the vanilla LSTM.\\nThe test set used are the 31 days of December of a single time-serie of the 261 time-series with measurements of the full year 2017.\\nMissing days are estimated.\\nNaive basemodels that are used are the mean forecast and the MAPE-minimization.\\nEvaluation metrics used: MSE, RMSE, NRMSE, MAE\\n####################################################\\n\\ncalculate the MSE on the total test set and for each day of the week.\\nShould normalize history and temperature with min max\\nRest: weekday, time, holiday use one hot encoder\\nStack different LSTM blocks --> hidden nodes serve as inputs for next LSTM's\\nUse finally a fully connected layer to generate an output\\nUse different learning rules: adam!!, stochastic gradient descent, Adagrad, Adadelta and RMSProp\\nAdam has parameters: learning rate, momentum and decay\\nplay with mini batches\\n\\nInputs:\\n1. history ok\\n2. temp ok\\n3. day week ok\\n4. time ok\\n5. holiday\\n6. (previous week future lag)\\n7. previous week error history lag --> how good are you following previous week\\njust the difference between the values\\n8. previous weeks load at the same moment in time and the temperatures of these days\\n\""
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd # pandas\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import casadi as ca\n",
    "from Test_basemodel_functions import *\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "plt.rc('axes', labelsize= 16)\n",
    "plt.rc('axes',titlesize = 18)\n",
    "plt.rc('legend',fontsize=14)\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('figure',figsize=(10,8))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# importing the data\n",
    "fullYeardata = pd.read_csv(\"D:\\Onedrive\\Leuven\\Final project\\data\\Forecasting_writtendata\\FullYear.csv\",index_col= \"date\",parse_dates= True)\n",
    "av_temperature = pd.read_csv(\"D:\\Onedrive\\Leuven\\Final project\\data\\weather-avg.csv\",index_col='meter_id')\n",
    "av_temperature = av_temperature.transpose()\n",
    "av_temperature.index = pd.to_datetime(av_temperature.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "name = fullYeardata.columns[0]\n",
    "TS = fullYeardata[name]\n",
    "temperature = av_temperature[name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# subsitute the missing values by prev week < prev day < mean of all the values at that time of the day (rarely used)\n",
    "def substitute_missing_values(TS: pd.Series):\n",
    "    for date in TS.index:\n",
    "        if np.isnan(TS[date]):\n",
    "            prev_week =  date + dt.timedelta(days=-7)\n",
    "            prev_day = date + dt.timedelta(days=-1)\n",
    "            if not np.isnan(TS[prev_week]):\n",
    "                TS[date] = TS[prev_week]\n",
    "\n",
    "            elif not np.isnan(TS[prev_day]):\n",
    "                TS[date] = TS[prev_day]\n",
    "\n",
    "            else:\n",
    "                temp = TS[TS.index.hour == date.hour]\n",
    "                data = temp[temp.index.minute == date.minute]\n",
    "                TS[date] = data.mean()\n",
    "    print(\"amount of missing values: %s. \\n\"%TS.isnull().sum())\n",
    "    return TS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of missing values: 0. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normalize the data --> min/max method (when using a single time serie)\n",
    "temperature_norm = norm(temperature)\n",
    "TS_norm = norm(TS)\n",
    "# temp = TS_norm[TS_norm.index.month != 12]\n",
    "# training = temp[temp != 11]\n",
    "# validation = TS_norm[TS_norm.index.month == 11]\n",
    "# test = TS_norm[TS_norm.index.month == 12]\n",
    "# remove from the test set all the days that contain nan values -> only estimate real days\n",
    "training = TS_norm[0:336]\n",
    "validation = TS_norm[336:384]\n",
    "test = TS_norm[384:432]\n",
    "test.dropna(inplace=True)\n",
    "# substitute the missing values\n",
    "training = substitute_missing_values(training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def get_av_temp(time_stamp: pd.Timestamp,temperature: pd.Series)-> float:\n",
    "    return temperature[temperature.index.dayofyear == time_stamp.dayofyear]\n",
    "\n",
    "def get_weekday(time_stamp: pd.Timestamp)-> int:\n",
    "    return time_stamp.weekday()\n",
    "\n",
    "def get_daytime(time_stamp: pd.Timestamp):\n",
    "    value = time_stamp.hour*2\n",
    "    if time_stamp.minute == 30:\n",
    "        value = value + 1\n",
    "    return value\n",
    "\n",
    "def is_holiday(time_stamp: pd.Timestamp, holidays: pd.DatetimeIndex)->bool:\n",
    "    return any(time_stamp.dayofyear == holidays.dayofyear)\n",
    "\n",
    "\n",
    "# forecast one time stamp at the time --> for loop to forecast the 48 time stamps\n",
    "def input_output_LSTM(training, temperature_norm, lag_value: int):\n",
    "    holidays = EnglandAndWalesHolidayCalendar().holidays(start=pd.Timestamp('2017-01-01'),end=pd.Timestamp('2017-12-31'))\n",
    "    amount_forecasts = len(training) - lag_value\n",
    "    amount_features = 1+1+7+48+2\n",
    "    X = np.zeros((amount_forecasts,lag_value,amount_features))\n",
    "    y = np.zeros((amount_forecasts,1))\n",
    "\n",
    "    for sample_id in np.arange(0,amount_forecasts):\n",
    "\n",
    "        history = training.values[sample_id:lag_value + sample_id]\n",
    "        y[sample_id] = training.values[lag_value + sample_id]\n",
    "        X[sample_id,:,0] = history\n",
    "        all_time_stamps = training.index[sample_id:lag_value + sample_id]\n",
    "        for lag_value_index in np.arange(0,lag_value): # goes from old to new\n",
    "            time_stamp = all_time_stamps[lag_value_index]\n",
    "\n",
    "            temperature_feed = get_av_temp(time_stamp,temperature_norm)\n",
    "            X[sample_id,lag_value_index,1] = temperature_feed\n",
    "\n",
    "            weekday = np.zeros(7)\n",
    "            day = get_weekday(time_stamp) # 0 - 6\n",
    "            weekday[day] = 1\n",
    "            X[sample_id,lag_value_index,2:9] = weekday\n",
    "\n",
    "            time = np.zeros(48)\n",
    "            time_of_day = get_daytime(time_stamp) # 0 - 47\n",
    "            time[time_of_day] = 1\n",
    "            X[sample_id,lag_value_index,9:57] = time\n",
    "\n",
    "            holiday = np.zeros(2)\n",
    "            hol = is_holiday(time_stamp,holidays) # [no, yes]\n",
    "            if hol:\n",
    "                holiday[1] = 1\n",
    "            else:\n",
    "                holiday[0] = 1\n",
    "            X[sample_id,lag_value_index,57:59] = holiday\n",
    "\n",
    "\n",
    "    return X,y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "from keras.layers import Dense,  LSTM,Embedding\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint, History\n",
    "\n",
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "modelckpt_callback = ModelCheckpoint(monitor=\"val_loss\",filepath=path_checkpoint,verbose=1,save_weights_only=True,save_best_only=True)\n",
    "def build_model(training: pd.Series,validation: pd.Series, temperature_norm: pd.Series, lag_value: int,nb_epoch: int = 1, regularization_parameter = 0.01,batch_size_para: int = 32,verbose_para: int = 1):\n",
    "    X,y = input_output_LSTM(training, temperature_norm, lag_value)\n",
    "    X_val,y_val = input_output_LSTM(validation, temperature_norm, lag_value)\n",
    "    regularizers.l2(l=regularization_parameter)\n",
    "    model = Sequential()\n",
    "    # no initial state is given --> hidden state are tensors filled with zeros\n",
    "    # dropout=None,recurrent_dropout=None\n",
    "    model.add(LSTM(units=20,activation='tanh',input_shape=(X.shape[1],X.shape[2])))\n",
    "    model.add(Dense(units=20,activation='relu',kernel_regularizer='l2'))\n",
    "    # model.add(Dropout(0.10))\n",
    "    model.add(Dense(units=y.shape[1],activation='relu',kernel_regularizer='l2'))\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    early_stopping_monitor = EarlyStopping(patience=2,restore_best_weights=True)\n",
    "    model.fit(x=X,y=y,epochs=nb_epoch,batch_size=batch_size_para,validation_data=(X_val,y_val),callbacks=[early_stopping_monitor,modelckpt_callback],verbose=verbose_para)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 333 samples, validate on 45 samples\n",
      "Epoch 1/150\n",
      "333/333 [==============================] - 1s 2ms/step - loss: 0.2188 - val_loss: 0.2060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20596, saving model to model_checkpoint.h5\n",
      "Epoch 2/150\n",
      "333/333 [==============================] - 0s 132us/step - loss: 0.2006 - val_loss: 0.1892\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20596 to 0.18924, saving model to model_checkpoint.h5\n",
      "Epoch 3/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.1844 - val_loss: 0.1743\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.18924 to 0.17428, saving model to model_checkpoint.h5\n",
      "Epoch 4/150\n",
      "333/333 [==============================] - 0s 102us/step - loss: 0.1696 - val_loss: 0.1603\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17428 to 0.16029, saving model to model_checkpoint.h5\n",
      "Epoch 5/150\n",
      "333/333 [==============================] - 0s 102us/step - loss: 0.1559 - val_loss: 0.1475\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16029 to 0.14754, saving model to model_checkpoint.h5\n",
      "Epoch 6/150\n",
      "333/333 [==============================] - 0s 114us/step - loss: 0.1434 - val_loss: 0.1357\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.14754 to 0.13571, saving model to model_checkpoint.h5\n",
      "Epoch 7/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.1318 - val_loss: 0.1248\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13571 to 0.12476, saving model to model_checkpoint.h5\n",
      "Epoch 8/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.1212 - val_loss: 0.1147\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12476 to 0.11472, saving model to model_checkpoint.h5\n",
      "Epoch 9/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.1115 - val_loss: 0.1055\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11472 to 0.10545, saving model to model_checkpoint.h5\n",
      "Epoch 10/150\n",
      "333/333 [==============================] - 0s 120us/step - loss: 0.1026 - val_loss: 0.0970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.10545 to 0.09696, saving model to model_checkpoint.h5\n",
      "Epoch 11/150\n",
      "333/333 [==============================] - 0s 123us/step - loss: 0.0944 - val_loss: 0.0891\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09696 to 0.08910, saving model to model_checkpoint.h5\n",
      "Epoch 12/150\n",
      "333/333 [==============================] - 0s 111us/step - loss: 0.0868 - val_loss: 0.0819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08910 to 0.08195, saving model to model_checkpoint.h5\n",
      "Epoch 13/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.0798 - val_loss: 0.0754\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08195 to 0.07538, saving model to model_checkpoint.h5\n",
      "Epoch 14/150\n",
      "333/333 [==============================] - 0s 120us/step - loss: 0.0735 - val_loss: 0.0693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07538 to 0.06932, saving model to model_checkpoint.h5\n",
      "Epoch 15/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.0676 - val_loss: 0.0637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.06932 to 0.06370, saving model to model_checkpoint.h5\n",
      "Epoch 16/150\n",
      "333/333 [==============================] - 0s 102us/step - loss: 0.0622 - val_loss: 0.0586\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.06370 to 0.05856, saving model to model_checkpoint.h5\n",
      "Epoch 17/150\n",
      "333/333 [==============================] - 0s 111us/step - loss: 0.0573 - val_loss: 0.0538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05856 to 0.05382, saving model to model_checkpoint.h5\n",
      "Epoch 18/150\n",
      "333/333 [==============================] - 0s 120us/step - loss: 0.0528 - val_loss: 0.0495\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05382 to 0.04951, saving model to model_checkpoint.h5\n",
      "Epoch 19/150\n",
      "333/333 [==============================] - 0s 123us/step - loss: 0.0487 - val_loss: 0.0456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.04951 to 0.04562, saving model to model_checkpoint.h5\n",
      "Epoch 20/150\n",
      "333/333 [==============================] - 0s 120us/step - loss: 0.0448 - val_loss: 0.0420\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.04562 to 0.04199, saving model to model_checkpoint.h5\n",
      "Epoch 21/150\n",
      "333/333 [==============================] - 0s 129us/step - loss: 0.0415 - val_loss: 0.0386\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.04199 to 0.03860, saving model to model_checkpoint.h5\n",
      "Epoch 22/150\n",
      "333/333 [==============================] - 0s 117us/step - loss: 0.0383 - val_loss: 0.0356\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.03860 to 0.03556, saving model to model_checkpoint.h5\n",
      "Epoch 23/150\n",
      "333/333 [==============================] - 0s 123us/step - loss: 0.0354 - val_loss: 0.0328\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.03556 to 0.03283, saving model to model_checkpoint.h5\n",
      "Epoch 24/150\n",
      "333/333 [==============================] - 0s 123us/step - loss: 0.0327 - val_loss: 0.0303\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.03283 to 0.03034, saving model to model_checkpoint.h5\n",
      "Epoch 25/150\n",
      "333/333 [==============================] - 0s 129us/step - loss: 0.0303 - val_loss: 0.0281\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.03034 to 0.02806, saving model to model_checkpoint.h5\n",
      "Epoch 26/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.0282 - val_loss: 0.0259\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02806 to 0.02589, saving model to model_checkpoint.h5\n",
      "Epoch 27/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.0262 - val_loss: 0.0242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02589 to 0.02420, saving model to model_checkpoint.h5\n",
      "Epoch 28/150\n",
      "333/333 [==============================] - 0s 111us/step - loss: 0.0243 - val_loss: 0.0225\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02420 to 0.02248, saving model to model_checkpoint.h5\n",
      "Epoch 29/150\n",
      "333/333 [==============================] - 0s 114us/step - loss: 0.0227 - val_loss: 0.0208\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.02248 to 0.02079, saving model to model_checkpoint.h5\n",
      "Epoch 30/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.0212 - val_loss: 0.0196\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.02079 to 0.01958, saving model to model_checkpoint.h5\n",
      "Epoch 31/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.0198 - val_loss: 0.0183\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01958 to 0.01830, saving model to model_checkpoint.h5\n",
      "Epoch 32/150\n",
      "333/333 [==============================] - 0s 117us/step - loss: 0.0185 - val_loss: 0.0173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01830 to 0.01727, saving model to model_checkpoint.h5\n",
      "Epoch 33/150\n",
      "333/333 [==============================] - 0s 108us/step - loss: 0.0174 - val_loss: 0.0162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01727 to 0.01619, saving model to model_checkpoint.h5\n",
      "Epoch 34/150\n",
      "333/333 [==============================] - 0s 111us/step - loss: 0.0164 - val_loss: 0.0154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01619 to 0.01535, saving model to model_checkpoint.h5\n",
      "Epoch 35/150\n",
      "333/333 [==============================] - 0s 126us/step - loss: 0.0154 - val_loss: 0.0147\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.01535 to 0.01468, saving model to model_checkpoint.h5\n",
      "Epoch 36/150\n",
      "333/333 [==============================] - 0s 168us/step - loss: 0.0147 - val_loss: 0.0138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.01468 to 0.01380, saving model to model_checkpoint.h5\n",
      "Epoch 37/150\n",
      "333/333 [==============================] - 0s 147us/step - loss: 0.0139 - val_loss: 0.0132\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.01380 to 0.01321, saving model to model_checkpoint.h5\n",
      "Epoch 38/150\n",
      "333/333 [==============================] - 0s 135us/step - loss: 0.0132 - val_loss: 0.0124\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.01321 to 0.01240, saving model to model_checkpoint.h5\n",
      "Epoch 39/150\n",
      "333/333 [==============================] - 0s 144us/step - loss: 0.0126 - val_loss: 0.0121\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.01240 to 0.01205, saving model to model_checkpoint.h5\n",
      "Epoch 40/150\n",
      "333/333 [==============================] - 0s 123us/step - loss: 0.0120 - val_loss: 0.0114\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01205 to 0.01142, saving model to model_checkpoint.h5\n",
      "Epoch 41/150\n",
      "333/333 [==============================] - 0s 129us/step - loss: 0.0116 - val_loss: 0.0111\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01142 to 0.01106, saving model to model_checkpoint.h5\n",
      "Epoch 42/150\n",
      "333/333 [==============================] - 0s 141us/step - loss: 0.0111 - val_loss: 0.0108\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01106 to 0.01078, saving model to model_checkpoint.h5\n",
      "Epoch 43/150\n",
      "333/333 [==============================] - 0s 111us/step - loss: 0.0108 - val_loss: 0.0102\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01078 to 0.01018, saving model to model_checkpoint.h5\n",
      "Epoch 44/150\n",
      "333/333 [==============================] - 0s 108us/step - loss: 0.0104 - val_loss: 0.0099\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.01018 to 0.00994, saving model to model_checkpoint.h5\n",
      "Epoch 45/150\n",
      "333/333 [==============================] - 0s 114us/step - loss: 0.0101 - val_loss: 0.0101\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00994\n",
      "Epoch 46/150\n",
      "333/333 [==============================] - 0s 129us/step - loss: 0.0097 - val_loss: 0.0096\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00994 to 0.00955, saving model to model_checkpoint.h5\n",
      "Epoch 47/150\n",
      "333/333 [==============================] - 0s 105us/step - loss: 0.0095 - val_loss: 0.0092\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00955 to 0.00921, saving model to model_checkpoint.h5\n",
      "Epoch 48/150\n",
      "333/333 [==============================] - 0s 114us/step - loss: 0.0093 - val_loss: 0.0092\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00921 to 0.00916, saving model to model_checkpoint.h5\n",
      "Epoch 49/150\n",
      "333/333 [==============================] - 0s 108us/step - loss: 0.0091 - val_loss: 0.0093\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00916\n",
      "Epoch 50/150\n",
      "333/333 [==============================] - 0s 132us/step - loss: 0.0089 - val_loss: 0.0089\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00916 to 0.00890, saving model to model_checkpoint.h5\n",
      "Epoch 51/150\n",
      "333/333 [==============================] - 0s 108us/step - loss: 0.0088 - val_loss: 0.0085\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00890 to 0.00851, saving model to model_checkpoint.h5\n",
      "Epoch 52/150\n",
      "333/333 [==============================] - 0s 120us/step - loss: 0.0085 - val_loss: 0.0090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00851\n",
      "Epoch 53/150\n",
      "333/333 [==============================] - 0s 108us/step - loss: 0.0085 - val_loss: 0.0091\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00851\n"
     ]
    }
   ],
   "source": [
    "history = build_model(training=training,validation=validation,temperature_norm=temperature_norm,lag_value=3,nb_epoch=150)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-78-535b04b038d2>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;31m# run the predictor ``warm'' on the trainingset --> before going to do predictions (remember matlab file)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhistory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"loss\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "# run the predictor ``warm'' on the trainingset --> before going to do predictions (remember matlab file)\n",
    "\n",
    "loss = history.history[\"loss\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-65-f0a4dd64a743>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m \u001B[0mvisualize_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Training and Validation Loss\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-65-f0a4dd64a743>\u001B[0m in \u001B[0;36mvisualize_loss\u001B[1;34m(history, title)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mvisualize_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtitle\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhistory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"loss\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m     \u001B[0mval_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhistory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"val_loss\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mepochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}