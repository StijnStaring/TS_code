{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\"\"\"\n",
    "This file is implementing the vanilla LSTM.\n",
    "The test set used are the 31 days of December of a single time-serie of the 261 time-series with measurements of the full year 2017.\n",
    "Missing days are estimated.\n",
    "Naive basemodels that are used are the mean forecast and the MAPE-minimization.\n",
    "Evaluation metrics used: MSE, RMSE, NRMSE, MAE\n",
    "####################################################\n",
    "\n",
    "calculate the MSE on the total test set and for each day of the week.\n",
    "Should normalize history and temperature with min max\n",
    "Rest: weekday, time, holiday use one hot encoder\n",
    "Stack different LSTM blocks --> hidden nodes serve as inputs for next LSTM's\n",
    "Use finally a fully connected layer to generate an output\n",
    "Use different learning rules: adam!!, stochastic gradient descent, Adagrad, Adadelta and RMSProp\n",
    "Adam has parameters: learning rate, momentum and decay\n",
    "play with mini batches\n",
    "\n",
    "Inputs:\n",
    "1. history ok\n",
    "2. temp ok\n",
    "3. day week ok\n",
    "4. time ok\n",
    "5. holiday\n",
    "6. (previous week future lag)\n",
    "7. previous week error history lag --> how good are you following previous week\n",
    "just the difference between the values\n",
    "8. previous weeks load at the same moment in time and the temperatures of these days\n",
    "\"\"\"\n"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nThis file is implementing the vanilla LSTM.\\nThe test set used are the 31 days of December of a single time-serie of the 261 time-series with measurements of the full year 2017.\\nMissing days are estimated.\\nNaive basemodels that are used are the mean forecast and the MAPE-minimization.\\nEvaluation metrics used: MSE, RMSE, NRMSE, MAE\\n####################################################\\n\\ncalculate the MSE on the total test set and for each day of the week.\\nShould normalize history and temperature with min max\\nRest: weekday, time, holiday use one hot encoder\\nStack different LSTM blocks --> hidden nodes serve as inputs for next LSTM's\\nUse finally a fully connected layer to generate an output\\nUse different learning rules: adam!!, stochastic gradient descent, Adagrad, Adadelta and RMSProp\\nAdam has parameters: learning rate, momentum and decay\\nplay with mini batches\\n\\nInputs:\\n1. history ok\\n2. temp ok\\n3. day week ok\\n4. time ok\\n5. holiday\\n6. (previous week future lag)\\n7. previous week error history lag --> how good are you following previous week\\njust the difference between the values\\n8. previous weeks load at the same moment in time and the temperatures of these days\\n\""
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd # pandas\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import casadi as ca\n",
    "from Test_basemodel_functions import *\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "plt.rc('axes', labelsize= 16)\n",
    "plt.rc('axes',titlesize = 18)\n",
    "plt.rc('legend',fontsize=14)\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('figure',figsize=(10,8))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# importing the data\n",
    "fullYeardata = pd.read_csv(\"D:\\Onedrive\\Leuven\\Final project\\data\\Forecasting_writtendata\\FullYear.csv\",index_col= \"date\",parse_dates= True)\n",
    "av_temperature = pd.read_csv(\"D:\\Onedrive\\Leuven\\Final project\\data\\weather-avg.csv\",index_col='meter_id')\n",
    "av_temperature = av_temperature.transpose()\n",
    "av_temperature.index = pd.to_datetime(av_temperature.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "name = fullYeardata.columns[0]\n",
    "TS = fullYeardata[name]\n",
    "temperature = av_temperature[name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# subsitute the missing values by prev week < prev day < mean of all the values at that time of the day (rarely used)\n",
    "def substitute_missing_values(TS: pd.Series):\n",
    "    for date in TS.index:\n",
    "        if np.isnan(TS[date]):\n",
    "            prev_week =  date + dt.timedelta(days=-7)\n",
    "            prev_day = date + dt.timedelta(days=-1)\n",
    "            if not np.isnan(TS[prev_week]):\n",
    "                TS[date] = TS[prev_week]\n",
    "\n",
    "            elif not np.isnan(TS[prev_day]):\n",
    "                TS[date] = TS[prev_day]\n",
    "\n",
    "            else:\n",
    "                temp = TS[TS.index.hour == date.hour]\n",
    "                data = temp[temp.index.minute == date.minute]\n",
    "                TS[date] = data.mean()\n",
    "    print(\"amount of missing values: %s. \\n\"%TS.isnull().sum())\n",
    "    return TS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of missing values: 0. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normalize the data --> min/max method (when using a single time serie)\n",
    "temperature_norm = norm(temperature)\n",
    "TS_norm = norm(TS)\n",
    "temp = TS_norm[TS_norm.index.month != 12]\n",
    "training = temp[temp != 11]\n",
    "validation = TS_norm[TS_norm.index.month == 11]\n",
    "test = TS_norm[TS_norm.index.month == 12]\n",
    "# remove from the test set all the days that contain nan values -> only estimate real days\n",
    "test.dropna(inplace=True)\n",
    "# substitute the missing values\n",
    "training = substitute_missing_values(training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def get_av_temp(time_stamp: pd.Timestamp,temperature: pd.Series)-> float:\n",
    "    return temperature[temperature.index.dayofyear == time_stamp.dayofyear]\n",
    "\n",
    "def get_weekday(time_stamp: pd.Timestamp)-> int:\n",
    "    return time_stamp.weekday()\n",
    "\n",
    "def get_daytime(time_stamp: pd.Timestamp):\n",
    "    value = time_stamp.hour*2\n",
    "    if time_stamp.minute == 30:\n",
    "        value = value + 1\n",
    "    return value\n",
    "\n",
    "def is_holiday(time_stamp: pd.Timestamp, holidays: pd.DatetimeIndex)->bool:\n",
    "    return time_stamp in holidays\n",
    "\n",
    "\n",
    "# forecast one time stamp at the time --> for loop to forecast the 48 time stamps\n",
    "def input_output_LSTM(training, temperature_norm, lag_value: int):\n",
    "    holidays = EnglandAndWalesHolidayCalendar().holidays(start=pd.Timestamp('2017-01-01'),end=pd.Timestamp('2017-12-31'))\n",
    "    amount_forecasts = len(training) - lag_value\n",
    "    # one sample corresponds \n",
    "\n",
    "    history = np.zeros((amount_forecasts,lag_value))\n",
    "    temperature_feed = np.zeros((amount_forecasts,1))\n",
    "    weekday = np.zeros((amount_forecasts,7))\n",
    "    time = np.zeros((amount_forecasts,48))\n",
    "    holiday = np.zeros((amount_forecasts,2))\n",
    "\n",
    "    y = training[lag_value:].values\n",
    "    y = y.reshape((3,1))\n",
    "\n",
    "\n",
    "    for sample_id in np.arange(0,amount_forecasts):\n",
    "        time_stamp = training.index[lag_value + sample_id]\n",
    "        history[sample_id,:] = training.values[sample_id:lag_value + sample_id]\n",
    "        temperature_feed[sample_id,:] = get_av_temp(time_stamp,temperature_norm)\n",
    "        day = get_weekday(time_stamp) # 0 - 6\n",
    "        weekday[sample_id,day] = 1\n",
    "        time_of_day = get_daytime(time_stamp) # 0 - 47\n",
    "        time[sample_id,time_of_day] = 1\n",
    "        hol = is_holiday(time_stamp,holidays) # [no, yes]\n",
    "        if hol:\n",
    "            holiday[sample_id,1] = 1\n",
    "        else:\n",
    "            holiday[sample_id,0] = 1\n",
    "\n",
    "    # horizontally stack the different arrays\n",
    "    X = np.hstack((history,temperature_feed,weekday,time,holiday))\n",
    "\n",
    "    return X,y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from keras.layers import Dense,  LSTM,Embedding\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "def build_model(training: pd.Series, temperature_norm: pd.Series, lag_value: int,regularization_parameter = 0.01,batch_size_para: int = 32,verbose_para: int = 2):\n",
    "    X,y = input_output_LSTM(training, temperature_norm, lag_value)\n",
    "    X_val,y_val = input_output_LSTM(validation, temperature_norm, lag_value)\n",
    "    regularizers.l2(l=regularization_parameter)\n",
    "    model = Sequential()\n",
    "    # no initial state is given --> hidden state are tensors filled with zeros\n",
    "\n",
    "    model.add(LSTM(units=20,activation='tanh',dropout=0,recurrent_dropout=0,input_shape=()))\n",
    "    model.add(Dense(units=20,activation='relu',kernel_regularizer='l2',input_shape=(X.shape[1],)))\n",
    "    # model.add(Dropout(0.10))\n",
    "    model.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "    early_stopping_monitor = EarlyStopping(patience=2,restore_best_weights=True)\n",
    "    model.fit(x=X,y=y,nb_epoch=150,batch_size=batch_size_para,validation_data=(X_val,y_val),callbacks=[early_stopping_monitor],verbose=verbose_para)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_2: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-53-70a8300977ad>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mbuild_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtemperature_norm\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m16\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-52-3e8503a6a572>\u001B[0m in \u001B[0;36mbuild_model\u001B[1;34m(training, temperature_norm, lag_value, regularization_parameter, batch_size_para, verbose_para)\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0mregularizers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0ml2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ml\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mregularization_parameter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSequential\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m     \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mLSTM\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mactivation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'tanh'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mrecurrent_dropout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mactivation\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'relu'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mkernel_regularizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'l2'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[1;31m# model.add(Dropout(0.10))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36madd\u001B[1;34m(self, layer)\u001B[0m\n\u001B[0;32m    164\u001B[0m                     \u001B[1;31m# and create the node connecting the current layer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m                     \u001B[1;31m# to the input layer we just created.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 166\u001B[1;33m                     \u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    167\u001B[0m                     \u001B[0mset_inputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    168\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs, initial_state, constants, **kwargs)\u001B[0m\n\u001B[0;32m    539\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    540\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0minitial_state\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mconstants\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 541\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mRNN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    542\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    543\u001B[0m         \u001B[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[0;32m    444\u001B[0m                 \u001B[1;31m# Raise exceptions in case the input is not compatible\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    445\u001B[0m                 \u001B[1;31m# with the input_spec specified in the layer constructor.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 446\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0massert_input_compatibility\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    447\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    448\u001B[0m                 \u001B[1;31m# Collect input shapes to build layer.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36massert_input_compatibility\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    340\u001B[0m                                      \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m': expected ndim='\u001B[0m \u001B[1;33m+\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    341\u001B[0m                                      \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m', found ndim='\u001B[0m \u001B[1;33m+\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 342\u001B[1;33m                                      str(K.ndim(x)))\n\u001B[0m\u001B[0;32m    343\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_ndim\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    344\u001B[0m                 \u001B[0mndim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mK\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Input 0 is incompatible with layer lstm_2: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# run the predictor ``warm'' on the trainingset\n",
    "build_model(training,temperature_norm,16)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}