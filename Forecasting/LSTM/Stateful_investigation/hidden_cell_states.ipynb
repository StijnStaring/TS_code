{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of missing values: 0. \n",
      "\n",
      "amount of missing values: 0. \n",
      "\n",
      "amount of missing values: 0. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.callbacks import History\n",
    "from forecasting_functions import *\n",
    "\n",
    "# importing the data\n",
    "Stijn = True\n",
    "fullYearpath = None\n",
    "av_temperaturepath = None\n",
    "\n",
    "if Stijn:\n",
    "    fullYearpath = \"D:\\Onedrive\\Leuven\\Final project\\data\\Forecasting_writtendata\\FullYear.csv\"\n",
    "    av_temperaturepath = \"D:\\Onedrive\\Leuven\\Final project\\data\\weather-avg.csv\"\n",
    "else:\n",
    "    raise Exception(\"Put some paths here.\")\n",
    "fullYeardata = pd.read_csv(fullYearpath,index_col= \"date\",parse_dates= True)\n",
    "av_temperature = pd.read_csv(av_temperaturepath,index_col='meter_id')\n",
    "av_temperature = av_temperature.transpose()\n",
    "av_temperature.index = pd.to_datetime(av_temperature.index)\n",
    "\n",
    "\n",
    "\n",
    "name = fullYeardata.columns[1]\n",
    "TS = fullYeardata[name]\n",
    "temperature = av_temperature[name]\n",
    "\n",
    "\n",
    "\n",
    "# all the training dates are present, dates from the test set can be missing.\n",
    "\n",
    "# normalize the data --> min/max method (when using a single time serie)\n",
    "temperature_norm,scaler_temperature = norm_forcast(temperature)\n",
    "temperature_norm = substitute_missing_values_temperature(temperature_norm)\n",
    "TS_norm,scaler_history = norm_forcast(TS)\n",
    "temp = TS_norm[TS_norm.index.month == 1]\n",
    "# temp = TS_norm[TS_norm.index.month != 12]\n",
    "# test = TS_norm[TS_norm.index.month == 12]\n",
    "\n",
    "# remove from the test set all the days that contain nan values -> only estimate real days\n",
    "# test.dropna(inplace=True)\n",
    "\n",
    "# substitute the missing values\n",
    "temp = substitute_missing_values(temp)\n",
    "training = temp[0:336]\n",
    "validation = temp[336:384]\n",
    "test = temp[384:528]\n",
    "# training = temp[temp.index.month != 11]\n",
    "# validation = temp[temp.index.month == 11]\n",
    "TS_norm_full = substitute_missing_values(TS_norm.copy(deep=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(333, 3, 59)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = input_output_LSTM(training, temperature_norm, 3)\n",
    "X_val, y_val = input_output_LSTM(validation, temperature_norm, 3)\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "----------\n",
      "[array([[0.09549879]], dtype=float32), array([[0.09549879]], dtype=float32), array([[0.19883052]], dtype=float32)]\n",
      "----------\n",
      "[array([[0.01529508]], dtype=float32), array([[0.01529508]], dtype=float32), array([[0.03055247]], dtype=float32)]\n",
      "----------\n",
      "[array([[0.00229115]], dtype=float32), array([[0.00229115]], dtype=float32), array([[0.00458128]], dtype=float32)]\n",
      "----------\n",
      "[array([[0.19473065]], dtype=float32), array([[0.19473065]], dtype=float32), array([[0.43662012]], dtype=float32)]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model\n",
    "inputs1 = Input(batch_shape=(1,3, 1))\n",
    "lstm1, state_h, state_c = LSTM(1, stateful= True, return_state= True)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n",
    "# define input data\n",
    "data = array([[0,0,0], [0.1, 0.2, 0.3],[0,0,0],[0,0,0],[0.6,0.3,0.6]])\n",
    "# make and show prediction\n",
    "for row in data:\n",
    "    row = row.reshape(1,3,1)\n",
    "    print(model.predict(row))\n",
    "    # model.reset_states()\n",
    "    print(10*'-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.        ],\n",
      "       [0.14619657],\n",
      "       [0.        ],\n",
      "       [0.        ],\n",
      "       [0.3154364 ]], dtype=float32), array([[0.        ],\n",
      "       [0.14619657],\n",
      "       [0.        ],\n",
      "       [0.        ],\n",
      "       [0.3154364 ]], dtype=float32), array([[0.        ],\n",
      "       [0.26257628],\n",
      "       [0.        ],\n",
      "       [0.        ],\n",
      "       [0.53567994]], dtype=float32)]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model\n",
    "inputs1 = Input(shape=(3, 1))\n",
    "lstm1, state_h, state_c = LSTM(1, return_state= True)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n",
    "# define input data\n",
    "data = array([[0,0,0], [0.1, 0.2, 0.3],[0,0,0],[0,0,0],[0.6,0.3,0.6]])\n",
    "data = data.reshape((5,3,1))\n",
    "# make and show prediction\n",
    "# for row in data:\n",
    "#     row = row.reshape(1,3,1)\n",
    "output = model.predict(data)\n",
    "print(model.predict(data))\n",
    "print(10*'-')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.05716937]], dtype=float32), array([[0.05716937]], dtype=float32), array([[0.11988591]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "\n",
    "# define model\n",
    "inputs1 = Input(shape=(3, 1))\n",
    "lstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n",
    "# define input data\n",
    "data = array([0.1, 0.2, 0.3]).reshape((1,3,1))\n",
    "# make and show prediction\n",
    "print(model.predict(data))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input() got an unexpected keyword argument 'input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-1b47e69c2886>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m# define model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0minputs1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[0mlstm1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mLSTM\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstateful\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mModel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlstm1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: Input() got an unexpected keyword argument 'input_shape'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model\n",
    "inputs1 = Input(shape=(3, 1))\n",
    "lstm1 = LSTM(1, stateful= False)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=lstm1)\n",
    "# define input data\n",
    "data = array([[0,0,0], [0.1, 0.2, 0.3],[0,0,0],[0,0,0],[0.6,0.3,0.6]])\n",
    "# make and show prediction\n",
    "# for row in data:\n",
    "#     row = row.reshape(5,3,1)\n",
    "print(model.predict(data.reshape((5,3,1)), batch_size= 5))\n",
    "# model.reset_states()\n",
    "print(10*'-')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "data = array([[0,0,0], [0.1, 0.2, 0.3],[0,0,0],[0,0,0],[0.6,0.3,0.6]])\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, batch_input_shape=(5,3,1)))\n",
    "\n",
    "old_weights = model.get_weights()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.\n- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-5177c72a43e2>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSequential\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_weights\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mold_weights\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mLSTM\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstateful\u001B[0m\u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_input_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;31m# for row in data:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\sequential.py\u001B[0m in \u001B[0;36madd\u001B[1;34m(self, layer)\u001B[0m\n\u001B[0;32m    164\u001B[0m                     \u001B[1;31m# and create the node connecting the current layer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m                     \u001B[1;31m# to the input layer we just created.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 166\u001B[1;33m                     \u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    167\u001B[0m                     \u001B[0mset_inputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    168\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs, initial_state, constants, **kwargs)\u001B[0m\n\u001B[0;32m    539\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    540\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0minitial_state\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mconstants\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 541\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mRNN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    542\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    543\u001B[0m         \u001B[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[0;32m    461\u001B[0m                                          \u001B[1;34m'You can build it manually via: '\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    462\u001B[0m                                          '`layer.build(batch_input_shape)`')\n\u001B[1;32m--> 463\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munpack_singleton\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_shapes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    464\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    465\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36mbuild\u001B[1;34m(self, input_shape)\u001B[0m\n\u001B[0;32m    520\u001B[0m                                for dim in state_size]\n\u001B[0;32m    521\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstateful\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 522\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_states\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    523\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    524\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgrammaX100\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\recurrent.py\u001B[0m in \u001B[0;36mreset_states\u001B[1;34m(self, states)\u001B[0m\n\u001B[0;32m    709\u001B[0m         \u001B[0mbatch_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_spec\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    710\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 711\u001B[1;33m             raise ValueError('If a RNN is stateful, it needs to know '\n\u001B[0m\u001B[0;32m    712\u001B[0m                              \u001B[1;34m'its batch size. Specify the batch size '\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    713\u001B[0m                              \u001B[1;34m'of your input tensors: \\n'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.\n- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer."
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "data = array([[0,0,0], [0.1, 0.2, 0.3],[0,0,0],[0,0,0],[0.6,0.3,0.6]])\n",
    "model = Sequential()\n",
    "model.set_weights(old_weights)\n",
    "model.add(LSTM(1, stateful= True, batch_input_shape=(None,3,1)))\n",
    "\n",
    "# for row in data:\n",
    "#     row = row.reshape(1,3,1)\n",
    "#     print(model.predict(row))\n",
    "#     # model.reset_states()\n",
    "#     print(10*'-')\n",
    "\n",
    "data = data[0:1].reshape((1,3,1))\n",
    "print(model.predict(data))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333 45\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,  LSTM,Embedding\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint, History\n",
    "\n",
    "def build_model_stateless1(training: pd.Series,validation: pd.Series, temperature_norm: pd.Series, lag_value: int,nb_epoch: int = 1, regularization_parameter: float = 0.01,batch_size_para: int = 32,verbose_para: int = 1, save: bool = False):\n",
    "    history = History()\n",
    "    X,y = input_output_LSTM(training, temperature_norm, lag_value)\n",
    "    X_val,y_val = input_output_LSTM(validation, temperature_norm, lag_value)\n",
    "    print(len(X), len(X_val))\n",
    "    regularizers.l2(l=regularization_parameter)\n",
    "    model = Sequential()\n",
    "    # no initial state is given --> hidden state are tensors filled with zeros\n",
    "    # dropout=None,recurrent_dropout=None\n",
    "    model.add(LSTM(units=1,activation='tanh',stateful= False, batch_input_shape=(3,X.shape[1],X.shape[2])))\n",
    "    # model.add(Dense(units=20,activation='relu',kernel_regularizer='l2'))\n",
    "    # model.add(Dropout(0.10))\n",
    "    # model.add(Dense(units=y.shape[1],activation='relu',kernel_regularizer='l2'))\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    early_stopping_monitor = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "    # shuffle false --> not shuffle the training data\n",
    "    # model.fit(x=X,y=y,validation_data=(X_val,y_val), epochs=nb_epoch,shuffle= False, batch_size=3,callbacks=[early_stopping_monitor,history],verbose=verbose_para)\n",
    "    # save the trained_model\n",
    "\n",
    "    return model\n",
    "\n",
    "trained_model = build_model_stateless1(training, validation,temperature_norm,3,3,verbose_para=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "weights = trained_model.get_weights()\n",
    "# trained_model.reset_states()\n",
    "# trained_model.set_weights(weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "trained_model.set_weights(weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07231653]]\n",
      "[[0.1699762]]\n",
      "[[0.07231653]]\n"
     ]
    }
   ],
   "source": [
    "X_pred = X\n",
    "X_pred = np.insert(X_pred,0,0,axis=0)\n",
    "X_pred = np.insert(X_pred,2,0,axis=0)\n",
    "X_pred = np.insert(X_pred,-1,0,axis=0)\n",
    "trained_model.reset_states()\n",
    "\n",
    "for i in range(3):\n",
    "    row = X_pred[i,:,:]\n",
    "    row = np.expand_dims(row,axis=0)\n",
    "    print(trained_model.predict(row, batch_size= 1))\n",
    "    trained_model.reset_states()\n",
    "\n",
    "\n",
    "# print(X_pred[0:3].shape)\n",
    "# print(trained_model.predict(X_pred[0:3], batch_size= 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}